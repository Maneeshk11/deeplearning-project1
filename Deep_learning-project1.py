# -*- coding: utf-8 -*-
"""Model Final Notebook

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TTSdisMpH76LQuSX16P77j_lK78-nw9e
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
from torchvision.datasets import CIFAR10
import pickle
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from torch.utils.data import random_split
import pandas as pd
from PIL import Image
import os
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import StepLR, MultiStepLR
from torch.utils.data import TensorDataset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
    transforms.RandomRotation(degrees=20),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

# Load full training set
full_trainset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)

# Split into training and validation sets (e.g., 80% train, 20% validation)
train_size = int(0.8 * len(full_trainset))
val_size = len(full_trainset) - train_size
trainset, valset = random_split(full_trainset, [train_size, val_size])

# Create data loaders
trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)
valloader = DataLoader(valset, batch_size=100, shuffle=False, num_workers=2)

testset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)
testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.skip = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.skip = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    def forward(self, x):
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.skip(x)
        out = torch.relu(out)
        return out

class CustomResNet(nn.Module):
    def __init__(self, num_classes=10):
        super(CustomResNet, self).__init__()
        self.init_conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.init_bn = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self._make_layer(64, 64, 4, stride=1)
        self.layer2 = self._make_layer(64, 128, 4, stride=2)
        self.layer3 = self._make_layer(128, 256, 3, stride=2)
        #self.layer4 = self._make_layer(256, 512, 2, stride=2)
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(256, num_classes)

    def _make_layer(self, in_channels, out_channels, blocks, stride):
        layers = [ResidualBlock(in_channels, out_channels, stride)]
        for _ in range(1, blocks):
            layers.append(ResidualBlock(out_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.init_conv(x)
        out = self.init_bn(out)
        out = self.relu(out)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        #out = self.layer4(out)
        out = self.avg_pool(out)
        out = torch.flatten(out, 1)
        out = self.fc(out)
        return out

num_epochs = 50

#numpy arrays of size 50 where each entry is the training accuracy, validation accuracy, train loss, validation loss of the model for that epoch
train_losses = np.zeros(num_epochs)
train_accuracies = np.zeros(num_epochs)
val_losses = np.zeros(num_epochs)
val_accuracies = np.zeros(num_epochs)




def train_model(model, train_loader, val_loader, epochs=num_epochs):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
    scheduler = MultiStepLR(optimizer, milestones=[30, 60, 80, 90], gamma=0.1)

    for epoch in range(num_epochs):
        # Training Phase
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        #calculate the training metrics: accuracy and loss
        train_loss = running_loss / len(train_loader)
        train_acc = 100 * correct / total

        # store trainign metrics in respective numpy array
        train_losses[epoch] = train_loss
        train_accuracies[epoch] = train_acc

        # Validation Phase
        model.eval()
        val_running_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_running_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        #Calculate validation metrics: loss and accurracy
        val_loss = val_running_loss/ len(val_loader)
        val_acc = 100 * correct / total

        #store validation metrics in respective numpy array
        val_losses[epoch] = val_loss
        val_accuracies[epoch] = val_acc

        scheduler.step()
        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')

model = CustomResNet().to(device)

# Print the number of parameters
from torchsummary import summary
summary(model, (3, 32, 32))

# Train the model
train_model(model, trainloader, valloader, epochs=50) #change epoch

torch.save('model.pth' , 'model.pth')

# visualize training and validation metrics per epoch


#loss plot
plt.figure(figsize=(8, 6))
plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Training Loss')
plt.plot(range(1, num_epochs+1), val_losses, 'r-', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('training_validation_loss.png')
plt.show()

# Accuracy plot
plt.figure(figsize=(8, 6))
plt.plot(range(1, num_epochs+1), train_accuracies, 'b-', label='Training Accuracy')
plt.plot(range(1, num_epochs+1), val_accuracies, 'r-', label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('training_validation_accuracy.png')
plt.show()

#Implementation of confuision matrix on entire cifar10 dataset

# CIFAR-10 class names
classes = ('plane', 'car', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')

#method that creates and then prints the generated confusion matrix

def plot_confusion_matrix(model, test_loader, device):
    # Initialize lists to store true labels and predictions
    y_true = []
    y_pred = []

    # Set model to evaluation mode
    model.eval()

    # Iterate through test data
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)

            # Get predictions
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)

            # Collect true labels and predictions
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Plot confusion matrix (single view with counts
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm,
        annot=True,           # Show the numbers in each cell
        fmt='d',              # Use integer format for numbers
        cmap='Blues',         # Use a blue color map like in your example
        xticklabels=classes,  # Use class names for x-axis
        yticklabels=classes   # Use class names for y-axis
    )
    plt.xlabel('Predicted Labels', fontsize=12)
    plt.ylabel('True Labels', fontsize=12)
    plt.title('Confusion Matrix', fontsize=14)

    # Improve the layout
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300)
    plt.show()

    # Calculate per-class accuracy
    per_class_accuracy = np.diag(cm) / np.sum(cm, axis=1)

    # Print per-class accuracy
    print("\nPer-class accuracy:")
    for i, class_name in enumerate(classes):
        print(f"{class_name}: {per_class_accuracy[i]:.2%}")

    # Print overall accuracy
    print(f"\nOverall accuracy: {np.sum(np.diag(cm)) / np.sum(cm):.2%}")

    return cm

#call method to print confusion matrix fuction
print("Generating confusion matrix...")
cm = plot_confusion_matrix(model, testloader, device)

with open("cifar_test_nolabel.pkl", "rb") as f:
    test_data = pickle.load(f)  # This is a dictionary

#Inspect the keys
print(test_data.keys())

model.eval()

# Load the test data
with open("cifar_test_nolabel.pkl", "rb") as f:
    test_data = pickle.load(f)  # This is a dictionary

# Inspect the keys
print("Keys in test data:", test_data.keys())

# Define transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])
])

# Convert the test data into a tensor and apply transformations
test_images = test_data[b"data"]  # Assuming key is 'data' and shape is (N, H, W, C)
test_images = torch.tensor(test_images, dtype=torch.uint8)  # Convert to tensor

# Permute to match (N, C, H, W)
test_images = test_images.permute(0, 3, 1, 2)

# Apply transformations
test_images = test_images.float() / 255.0  # Convert to float and normalize
test_images = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])(test_images)

# Move to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
test_images = test_images.to(device)

# Run model predictions
with torch.no_grad():
    test_outputs = model(test_images)
    predictions = test_outputs.argmax(dim=1).cpu().numpy()  # Get predicted labels

# Generate submission file
submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'Labels': predictions})
submission.to_csv('submission1.csv', index=False)
print("Submission1 file saved.")