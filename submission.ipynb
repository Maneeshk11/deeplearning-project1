{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBXdbtIGJw_j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR, MultiStepLR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert numpy array to PIL Image\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness = 0.1,contrast = 0.1,saturation = 0.1),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor = 2,p = 0.2),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "    transforms.RandomErasing(p=0.2,scale=(0.02, 0.1),value=1.0, inplace=False)\n",
        "])"
      ],
      "metadata": {
        "id": "-5pNpKlxLjSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"cifar_test_nolabel.pkl\", \"rb\") as f:\n",
        "    test_data = pickle.load(f)  # This is a dictionary\n",
        "\n",
        "# Inspect the keys\n",
        "print(test_data.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTUIqq4fMWKQ",
        "outputId": "b7ddf9fb-262f-4302-d33e-fb24bbfa013e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([b'data', b'ids'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "full_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "\n",
        "# Train-Validation Split\n",
        "train_size = int(0.8 * len(full_dataset))  # 80% for training\n",
        "val_size = len(full_dataset) - train_size  # 20% for validation\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False, num_workers=4)  # Larger batch size for validation\n",
        "\n",
        "train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # Convert to HWC format\n",
        "train_labels = np.array(train_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "TMLVwkOrL2ZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4faf95c5-4ff4-4cfe-c326-26aeb58109ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "\n",
        "print(type(full_dataset))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ncSAxXxiCD6",
        "outputId": "5e01a8cd-29c0-483b-ca97-86b82d920f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:12<00:00, 13.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "<class 'torchvision.datasets.cifar.CIFAR10'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "full_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "# Convert to dictionary\n",
        "cifar_dict = {\n",
        "    \"images\": [],  # Store images\n",
        "    \"labels\": []   # Store labels\n",
        "}\n",
        "\n",
        "# Iterate over dataset to extract images and labels\n",
        "for img, label in full_dataset:\n",
        "    cifar_dict[\"images\"].append(img)  # Append image (as Tensor)\n",
        "    cifar_dict[\"labels\"].append(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEVTPyL9jXwO",
        "outputId": "f6a9d91d-f5b4-414b-87e6-76028d654935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "for img in cifar_dict[\"images\"]:\n",
        "    train_data.append(img)\n",
        "\n",
        "for label in cifar_dict[\"labels\"]:\n",
        "    train_labels.append(label)\n",
        "\n",
        "train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # Convert to HWC format\n",
        "train_labels = np.array(train_labels)"
      ],
      "metadata": {
        "id": "SLu6DK_Glqjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert numpy array to PIL Image\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness = 0.1,contrast = 0.1,saturation = 0.1),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor = 2,p = 0.2),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "    transforms.RandomErasing(p=0.2,scale=(0.02, 0.1),value=1.0, inplace=False)\n",
        "])\n",
        "\n",
        "# Convert to TensorDataset and apply transformations\n",
        "class CustomCIFAR10Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "full_train_dataset = CustomCIFAR10Dataset(train_data, train_labels, transform=train_transform)"
      ],
      "metadata": {
        "id": "f70bcsf1mOJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(full_dataset))  # 80% for training\n",
        "val_size = len(full_dataset) - train_size  # 20% for validation\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbxp6pVHnlto",
        "outputId": "ef17c6e4-bae5-452e-fe10-5894e984b540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"cifar_test_nolabel.pkl\", \"rb\") as f:\n",
        "    test_data = pickle.load(f)  # Dictionary with byte keys\n",
        "\n",
        "# Convert byte keys to normal strings (if needed)\n",
        "test_images = test_data[b'data']  # Extract images\n",
        "test_ids = test_data[b'ids']  # Extract corresponding image IDs\n",
        "\n",
        "# Convert images from NumPy array to PyTorch tensor\n",
        "test_images = torch.tensor(np.array(test_images), dtype=torch.float32)  # Shape: (N, C, H, W)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Normalize the images\n",
        "test_images = transform(test_images / 255.0)\n",
        "\n",
        "# Create a DataLoader for testing (no labels)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_images)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "Sk6lizi5huZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=50):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[30, 60, 80, 90], gamma=0.1)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = 100 * correct / total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n"
      ],
      "metadata": {
        "id": "hNWR9e6rpPvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.skip = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.skip:\n",
        "            identity = self.skip(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "it8V3ZRDqzUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CustomResNet, self).__init__()\n",
        "        self.init_conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.init_bn = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(64, 64, 4, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, 4, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 3, stride=2)\n",
        "        #self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
        "        layers = [ResidualBlock(in_channels, out_channels, stride)]\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.init_conv(x)\n",
        "        out = self.init_bn(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        #out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "eCZx9TZ4q27A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomResNet().to(device)\n",
        "\n",
        "# Print the number of parameters\n",
        "from torchsummary import summary\n",
        "summary(model, (3, 32, 32))\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, epochs=20) #change epoch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlUftoOyq6ma",
        "outputId": "fe286bcf-1828-418e-8c63-cbc1a435c0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "            Conv2d-7           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-8           [-1, 64, 32, 32]             128\n",
            "              ReLU-9           [-1, 64, 32, 32]               0\n",
            "    ResidualBlock-10           [-1, 64, 32, 32]               0\n",
            "           Conv2d-11           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-12           [-1, 64, 32, 32]             128\n",
            "             ReLU-13           [-1, 64, 32, 32]               0\n",
            "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
            "             ReLU-16           [-1, 64, 32, 32]               0\n",
            "    ResidualBlock-17           [-1, 64, 32, 32]               0\n",
            "           Conv2d-18           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-19           [-1, 64, 32, 32]             128\n",
            "             ReLU-20           [-1, 64, 32, 32]               0\n",
            "           Conv2d-21           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
            "             ReLU-23           [-1, 64, 32, 32]               0\n",
            "    ResidualBlock-24           [-1, 64, 32, 32]               0\n",
            "           Conv2d-25           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-26           [-1, 64, 32, 32]             128\n",
            "             ReLU-27           [-1, 64, 32, 32]               0\n",
            "           Conv2d-28           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-29           [-1, 64, 32, 32]             128\n",
            "             ReLU-30           [-1, 64, 32, 32]               0\n",
            "    ResidualBlock-31           [-1, 64, 32, 32]               0\n",
            "           Conv2d-32          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-33          [-1, 128, 16, 16]             256\n",
            "           Conv2d-34          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-35          [-1, 128, 16, 16]             256\n",
            "             ReLU-36          [-1, 128, 16, 16]               0\n",
            "           Conv2d-37          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
            "             ReLU-39          [-1, 128, 16, 16]               0\n",
            "    ResidualBlock-40          [-1, 128, 16, 16]               0\n",
            "           Conv2d-41          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-42          [-1, 128, 16, 16]             256\n",
            "             ReLU-43          [-1, 128, 16, 16]               0\n",
            "           Conv2d-44          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-45          [-1, 128, 16, 16]             256\n",
            "             ReLU-46          [-1, 128, 16, 16]               0\n",
            "    ResidualBlock-47          [-1, 128, 16, 16]               0\n",
            "           Conv2d-48          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-49          [-1, 128, 16, 16]             256\n",
            "             ReLU-50          [-1, 128, 16, 16]               0\n",
            "           Conv2d-51          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 16, 16]             256\n",
            "             ReLU-53          [-1, 128, 16, 16]               0\n",
            "    ResidualBlock-54          [-1, 128, 16, 16]               0\n",
            "           Conv2d-55          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-56          [-1, 128, 16, 16]             256\n",
            "             ReLU-57          [-1, 128, 16, 16]               0\n",
            "           Conv2d-58          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-59          [-1, 128, 16, 16]             256\n",
            "             ReLU-60          [-1, 128, 16, 16]               0\n",
            "    ResidualBlock-61          [-1, 128, 16, 16]               0\n",
            "           Conv2d-62            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-63            [-1, 256, 8, 8]             512\n",
            "           Conv2d-64            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-65            [-1, 256, 8, 8]             512\n",
            "             ReLU-66            [-1, 256, 8, 8]               0\n",
            "           Conv2d-67            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-68            [-1, 256, 8, 8]             512\n",
            "             ReLU-69            [-1, 256, 8, 8]               0\n",
            "    ResidualBlock-70            [-1, 256, 8, 8]               0\n",
            "           Conv2d-71            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-72            [-1, 256, 8, 8]             512\n",
            "             ReLU-73            [-1, 256, 8, 8]               0\n",
            "           Conv2d-74            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-75            [-1, 256, 8, 8]             512\n",
            "             ReLU-76            [-1, 256, 8, 8]               0\n",
            "    ResidualBlock-77            [-1, 256, 8, 8]               0\n",
            "           Conv2d-78            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-79            [-1, 256, 8, 8]             512\n",
            "             ReLU-80            [-1, 256, 8, 8]               0\n",
            "           Conv2d-81            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-82            [-1, 256, 8, 8]             512\n",
            "             ReLU-83            [-1, 256, 8, 8]               0\n",
            "    ResidualBlock-84            [-1, 256, 8, 8]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 256, 1, 1]               0\n",
            "           Linear-86                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 4,697,162\n",
            "Trainable params: 4,697,162\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 25.88\n",
            "Params size (MB): 17.92\n",
            "Estimated Total Size (MB): 43.81\n",
            "----------------------------------------------------------------\n",
            "Epoch 1, Train Loss: 1.8710, Train Acc: 31.05%, Val Loss: 1.8311, Val Acc: 36.58%\n",
            "Epoch 2, Train Loss: 1.4979, Train Acc: 44.63%, Val Loss: 1.4177, Val Acc: 49.09%\n",
            "Epoch 3, Train Loss: 1.2907, Train Acc: 52.98%, Val Loss: 1.2954, Val Acc: 53.08%\n",
            "Epoch 4, Train Loss: 1.1391, Train Acc: 59.16%, Val Loss: 1.2195, Val Acc: 55.11%\n",
            "Epoch 5, Train Loss: 1.0128, Train Acc: 63.87%, Val Loss: 1.0008, Val Acc: 64.74%\n",
            "Epoch 6, Train Loss: 0.9160, Train Acc: 67.31%, Val Loss: 1.0682, Val Acc: 62.87%\n",
            "Epoch 7, Train Loss: 0.8209, Train Acc: 71.28%, Val Loss: 0.9550, Val Acc: 67.08%\n",
            "Epoch 8, Train Loss: 0.7435, Train Acc: 74.05%, Val Loss: 0.7895, Val Acc: 72.26%\n",
            "Epoch 9, Train Loss: 0.6755, Train Acc: 76.47%, Val Loss: 0.6861, Val Acc: 75.37%\n",
            "Epoch 10, Train Loss: 0.6263, Train Acc: 78.14%, Val Loss: 0.6910, Val Acc: 75.79%\n",
            "Epoch 11, Train Loss: 0.5749, Train Acc: 79.95%, Val Loss: 0.5730, Val Acc: 79.89%\n",
            "Epoch 12, Train Loss: 0.5333, Train Acc: 81.56%, Val Loss: 0.6002, Val Acc: 79.81%\n",
            "Epoch 13, Train Loss: 0.5024, Train Acc: 82.30%, Val Loss: 0.5543, Val Acc: 80.59%\n",
            "Epoch 14, Train Loss: 0.4684, Train Acc: 83.71%, Val Loss: 0.5541, Val Acc: 81.00%\n",
            "Epoch 15, Train Loss: 0.4422, Train Acc: 84.64%, Val Loss: 0.5113, Val Acc: 82.79%\n",
            "Epoch 16, Train Loss: 0.4220, Train Acc: 85.41%, Val Loss: 0.5645, Val Acc: 80.48%\n",
            "Epoch 17, Train Loss: 0.4021, Train Acc: 86.01%, Val Loss: 0.5647, Val Acc: 81.16%\n",
            "Epoch 18, Train Loss: 0.3798, Train Acc: 86.89%, Val Loss: 0.5333, Val Acc: 81.41%\n",
            "Epoch 19, Train Loss: 0.3583, Train Acc: 87.49%, Val Loss: 0.5367, Val Acc: 81.97%\n",
            "Epoch 20, Train Loss: 0.3460, Train Acc: 87.94%, Val Loss: 0.5226, Val Acc: 82.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        images = batch[0].to(device)  # Get images tensor from tuple and move to device\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Generate submission file\n",
        "submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'Labels': predictions})\n",
        "submission.to_csv('submission1.csv', index=False)\n",
        "print(\"Submission1 file saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h3IEmwyvdaQ",
        "outputId": "ebf0e9a6-8699-4940-8889-7f53e74bad17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission1 file saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.model = models.resnet18(weights=None)  # Training from scratch\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)  # Modify final layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "1l2iJZ7Japfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet18(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce LR every 10 epochs\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = 100.0 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzkRo4Qza3RL",
        "outputId": "4bfcd93b-b7f9-4b14-f995-2da083f6da20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 1.7384, Train Acc: 42.06%\n",
            "Epoch [2/50], Loss: 1.4725, Train Acc: 55.57%\n",
            "Epoch [3/50], Loss: 1.3533, Train Acc: 61.34%\n",
            "Epoch [4/50], Loss: 1.2770, Train Acc: 64.94%\n",
            "Epoch [5/50], Loss: 1.2280, Train Acc: 67.55%\n",
            "Epoch [6/50], Loss: 1.1833, Train Acc: 69.55%\n",
            "Epoch [7/50], Loss: 1.1515, Train Acc: 71.09%\n",
            "Epoch [8/50], Loss: 1.1187, Train Acc: 72.43%\n",
            "Epoch [9/50], Loss: 1.1016, Train Acc: 73.31%\n",
            "Epoch [10/50], Loss: 1.0757, Train Acc: 74.36%\n",
            "Epoch [11/50], Loss: 1.0581, Train Acc: 75.50%\n",
            "Epoch [12/50], Loss: 1.0418, Train Acc: 76.05%\n",
            "Epoch [13/50], Loss: 1.0311, Train Acc: 76.47%\n",
            "Epoch [14/50], Loss: 1.0149, Train Acc: 77.38%\n",
            "Epoch [15/50], Loss: 1.0085, Train Acc: 77.54%\n",
            "Epoch [16/50], Loss: 0.9922, Train Acc: 78.28%\n",
            "Epoch [17/50], Loss: 0.9831, Train Acc: 78.78%\n",
            "Epoch [18/50], Loss: 0.9757, Train Acc: 79.19%\n",
            "Epoch [19/50], Loss: 0.9637, Train Acc: 79.77%\n",
            "Epoch [20/50], Loss: 0.9628, Train Acc: 79.57%\n",
            "Epoch [21/50], Loss: 0.9522, Train Acc: 80.21%\n",
            "Epoch [22/50], Loss: 0.9415, Train Acc: 80.66%\n",
            "Epoch [23/50], Loss: 0.9379, Train Acc: 80.78%\n",
            "Epoch [24/50], Loss: 0.9305, Train Acc: 81.11%\n",
            "Epoch [25/50], Loss: 0.9250, Train Acc: 81.39%\n",
            "Epoch [26/50], Loss: 0.9217, Train Acc: 81.44%\n",
            "Epoch [27/50], Loss: 0.9164, Train Acc: 81.88%\n",
            "Epoch [28/50], Loss: 0.9089, Train Acc: 82.29%\n",
            "Epoch [29/50], Loss: 0.9158, Train Acc: 81.84%\n",
            "Epoch [30/50], Loss: 0.9004, Train Acc: 82.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (images,) in enumerate(test_loader):  # No labels, only images\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)  # Get class with highest probability\n",
        "\n",
        "        # Compute correct batch start index\n",
        "        batch_start = idx * test_loader.batch_size\n",
        "        for i, pred in enumerate(predicted.cpu().numpy()):\n",
        "            predictions.append((test_ids[batch_start + i], int(pred)))  # Convert prediction to int\n",
        "\n",
        "# Convert predictions to DataFrame\n",
        "df = pd.DataFrame(predictions, columns=[\"ID\", \"Labels\"])\n",
        "\n",
        "# Save as CSV file\n",
        "df.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"Predictions saved to submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2Xb96ZIuMZP",
        "outputId": "57c84266-c85d-46a1-e41b-052f33a9fbaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to submission.csv\n"
          ]
        }
      ]
    }
  ]
}